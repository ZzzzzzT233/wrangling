{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "13ad028b-72b7-43ed-aa78-96fd4e518040",
      "metadata": {
        "id": "13ad028b-72b7-43ed-aa78-96fd4e518040"
      },
      "source": [
        "# Assignment: Data Wrangling\n",
        "## `! git clone https://github.com/DS3001/wrangling`\n",
        "## Do Q2, and one of Q1 or Q3."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5735a4d4-8be8-433a-a351-70eb8002e632",
      "metadata": {
        "id": "5735a4d4-8be8-433a-a351-70eb8002e632"
      },
      "source": [
        "**Q1.** Open the \"tidy_data.pdf\" document in the repo, which is a paper called Tidy Data by Hadley Wickham.\n",
        "\n",
        "  1. Read the abstract. What is this paper about?\n",
        "  2. Read the introduction. What is the \"tidy data standard\" intended to accomplish?\n",
        "  3. Read the intro to section 2. What does this sentence mean: \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way.\" What does this sentence mean: \"For a given dataset, it’s usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general.\"\n",
        "  4. Read Section 2.2. How does Wickham define values, variables, and observations?\n",
        "  5. How is \"Tidy Data\" defined in section 2.3?\n",
        "  6. Read the intro to Section 3 and Section 3.1. What are the 5 most common problems with messy datasets? Why are the data in Table 4 messy? What is \"melting\" a dataset?\n",
        "  7. Why, specifically, is table 11 messy but table 12 tidy and \"molten\"?\n",
        "  8. Read Section 6. What is the \"chicken-and-egg\" problem with focusing on tidy data? What does Wickham hope happens in the future with further work on the subject of data wrangling?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d750da4c",
      "metadata": {},
      "source": [
        "1. The paper is about data tidying, which is an important aspect regarding data cleaning. Considering the huge amount of time that most people spent on cleaning data, the author points out the necessity to have a consistant data structure corresponding with minimal nubmers of tools.\n",
        "2. \"tidy data standard\" is intended to provide a standard, unified ways to organize data. Realizing the huge amount of time taken to clean the data, Wickham decides to provide a standard way to shorten the necessary time organizing the data and the effort needed to handle the connections between different steps.\n",
        "3. The first sentence means that tidy datasets follow a consistent structure, making them easier to work with, while messy datasets are unpredictable and unique in their problems. The second sentence says that while it's usually easy to identify variables and observations in a specific dataset, it’s hard to define them universally because each dataset has its own context.\n",
        "4. Wickham defines values as individual data points, variables as the things being measured, such as temperature, and observations as the set of values related to a single unit (like a person or day).\n",
        "5. \"Tidy data\" is defined as each variable is a column, each observation is a row, and each type of observational unit gets its own table. \n",
        "6. Messy data often has column headers that should be values, multiple variables crammed into one column, variables spread out in both rows and columns, different types of data mixed into one table, or data about one unit split across multiple tables. Table 4 is messy because the income categories are stuck in the column headers instead of being treated as values. Melting fixes this by converting those columns into rows and making the dataset more workable.\n",
        "7. Table 11 is a mess because it stores tmin and tmax as rows instead of columns, which breaks the tidy data structure. In contrast, Table 12 is tidy because it puts everything into proper columns, making it easier to analyze.\n",
        "8. The \"chicken-and-egg\" issue is that tidy data is only as useful as the tools we have to work with it. Wickham hopes that as more tools are built around tidy data, the process of working with and cleaning data will get even easier, pushing the field forward."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072",
      "metadata": {
        "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072"
      },
      "source": [
        "**Q2.** This question provides some practice cleaning variables which have common problems.\n",
        "1. Numeric variable: For `./data/airbnb_hw.csv`, clean the `Price` variable as well as you can, and explain the choices you make. How many missing values do you end up with? (Hint: What happens to the formatting when a price goes over 999 dollars, say from 675 to 1,112?)\n",
        "2. Categorical variable: For the `./data/sharks.csv` data covered in the lecture, clean the \"Type\" variable as well as you can, and explain the choices you make.\n",
        "3. Dummy variable: For the pretrial data covered in the lecture, clean the `WhetherDefendantWasReleasedPretrial` variable as well as you can, and, in particular, replace missing values with `np.nan`.\n",
        "4. Missing values, not at random: For the pretrial data covered in the lecture, clean the `ImposedSentenceAllChargeInContactEvent` variable as well as you can, and explain the choices you make. (Hint: Look at the `SentenceTypeAllChargesAtConvictionInContactEvent` variable.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "50f0c80d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Host Id                         int64\n",
            "Host Since                     object\n",
            "Name                           object\n",
            "Neighbourhood                  object\n",
            "Property Type                  object\n",
            "Review Scores Rating (bin)    float64\n",
            "Room Type                      object\n",
            "Zipcode                       float64\n",
            "Beds                          float64\n",
            "Number of Records               int64\n",
            "Number Of Reviews               int64\n",
            "Price                          object\n",
            "Review Scores Rating          float64\n",
            "dtype: object \n",
            "\n",
            "['145' '37' '28' '199' '549' '149' '250' '90' '270' '290' '170' '59' '49'\n",
            " '68' '285' '75' '100' '150' '700' '125' '175' '40' '89' '95' '99' '499'\n",
            " '120' '79' '110' '180' '143' '230' '350' '135' '85' '60' '70' '55' '44'\n",
            " '200' '165' '115' '74' '84' '129' '50' '185' '80' '190' '140' '45' '65'\n",
            " '225' '600' '109' '1,990' '73' '240' '72' '105' '155' '160' '42' '132'\n",
            " '117' '295' '280' '159' '107' '69' '239' '220' '399' '130' '375' '585'\n",
            " '275' '139' '260' '35' '133' '300' '289' '179' '98' '195' '29' '27' '39'\n",
            " '249' '192' '142' '169' '1,000' '131' '138' '113' '122' '329' '101' '475'\n",
            " '238' '272' '308' '126' '235' '315' '248' '128' '56' '207' '450' '215'\n",
            " '210' '385' '445' '136' '247' '118' '77' '76' '92' '198' '205' '299'\n",
            " '222' '245' '104' '153' '349' '114' '320' '292' '226' '420' '500' '325'\n",
            " '307' '78' '265' '108' '123' '189' '32' '58' '86' '219' '800' '335' '63'\n",
            " '229' '425' '67' '87' '1,200' '158' '650' '234' '310' '695' '400' '166'\n",
            " '119' '62' '168' '340' '479' '43' '395' '144' '52' '47' '529' '187' '209'\n",
            " '233' '82' '269' '163' '172' '305' '156' '550' '435' '137' '124' '48'\n",
            " '279' '330' '5,000' '134' '378' '97' '277' '64' '193' '147' '186' '264'\n",
            " '30' '3,000' '112' '94' '379' '57' '415' '236' '410' '214' '88' '66' '71'\n",
            " '171' '157' '545' '1,500' '83' '96' '1,800' '81' '188' '380' '255' '505'\n",
            " '54' '33' '174' '93' '740' '640' '1,300' '440' '599' '357' '1,239' '495'\n",
            " '127' '5,999' '178' '348' '152' '242' '183' '253' '750' '259' '365' '273'\n",
            " '197' '397' '103' '389' '355' '559' '38' '203' '999' '141' '162' '333'\n",
            " '698' '46' '360' '895' '10' '41' '206' '281' '449' '388' '212' '102'\n",
            " '201' '2,750' '4,750' '432' '675' '167' '390' '298' '339' '194' '302'\n",
            " '211' '595' '191' '53' '361' '480' '8,000' '4,500' '459' '997' '345'\n",
            " '216' '218' '111' '735' '276' '91' '490' '850' '398' '36' '775' '267'\n",
            " '625' '336' '2,500' '176' '725' '3,750' '469' '106' '460' '287' '575'\n",
            " '227' '263' '25' '228' '208' '177' '880' '148' '116' '685' '470' '217'\n",
            " '164' '61' '645' '699' '405' '252' '319' '268' '419' '343' '525' '311'\n",
            " '840' '154' '294' '950' '409' '184' '257' '204' '241' '2,000' '412' '121'\n",
            " '288' '196' '900' '647' '524' '1,750' '309' '510' '1,495' '1,700' '799'\n",
            " '383' '372' '492' '327' '1,999' '656' '224' '173' '875' '1,170' '795'\n",
            " '690' '146' '465' '1,100' '151' '274' '429' '825' '282' '256' '1,111'\n",
            " '620' '271' '161' '51' '855' '579' '1,174' '430' '20' '899' '649' '485'\n",
            " '181' '455' '4,000' '243' '342' '590' '560' '374' '437' '232' '359' '985'\n",
            " '31' '244' '254' '723' '237' '428' '370' '34' '1,400' '580' '2,520' '221'\n",
            " '749' '1,600' '2,695' '306' '202' '680' '570' '520' '223' '2,295' '213'\n",
            " '1,065' '346' '24' '286' '296' '266' '26' '995' '1,368' '393' '182' '635'\n",
            " '258' '780' '589' '347' '1,250' '1,350' '446' '3,200' '1,050' '1,650'\n",
            " '1,550' '975' '323' '6,500' '2,499' '1,850' '2,250' '715' '461' '540'\n",
            " '356' '439' '384' '569' '1,900' '22' '785' '626' '830' '318' '444' '321'\n",
            " '401' '1,499' '888' '369' '770' '386' '366' '344' '630' '313' '597' '262'\n",
            " '509' '10,000' '278' '312' '789' '1,195' '422' '21' '765' '3,500' '945'\n",
            " '326' '3,100' '2,486' '3,390' '1,356' '2,599' '472' '454' '328' '396'\n",
            " '291'] \n",
            "\n",
            "Total Missings: 0 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "#Q2-1\n",
        "df = pd.read_csv('./data/airbnb_hw.csv',low_memory=False)\n",
        "df.head()\n",
        "print(df.dtypes, '\\n')\n",
        "print(df[\"Price\"].unique(),'\\n')\n",
        "\n",
        "num_price = df[\"Price\"].str.replace(\",\", \"\")\n",
        "num_price = pd.to_numeric(num_price, errors=\"coerce\")\n",
        "print(\"Total Missings:\", sum(num_price.isnull()),\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5cf7db3",
      "metadata": {},
      "source": [
        "I start with using \"dtype\" to check the types of the variables and I found out that the type of Price is object instead of integer. Then I looked up the values of elements in Price using \"unique\" and I found out that the type of Price is string and there is a comma seperator for the thousands, which stops me from directly using \"to_numeric\" to transfer the type from string to integer. Therefore, I used \"str.replace\" to remove the comma seperator first and then did the transformation. Finally, I used \"isnull\" to sum all the null values in order to find the nubmer of missing values. The output showed no value is missing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "c1885144",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "index             int64\n",
            "Case Number      object\n",
            "Date             object\n",
            "Year            float64\n",
            "Type             object\n",
            "                 ...   \n",
            "Unnamed: 251    float64\n",
            "Unnamed: 252    float64\n",
            "Unnamed: 253    float64\n",
            "Unnamed: 254    float64\n",
            "Unnamed: 255    float64\n",
            "Length: 257, dtype: object \n",
            "\n",
            "['Unprovoked' 'Provoked' 'Questionable' 'Watercraft' 'Unconfirmed'\n",
            " 'Unverified' 'Invalid' 'Under investigation' 'Boating' 'Sea Disaster' nan\n",
            " 'Boat' 'Boatomg'] \n",
            "\n",
            "['Unprovoked' 'Provoked' nan 'Watercraft_Related'] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Q2-2\n",
        "df = pd.read_csv('./data/sharks.csv',low_memory=False)\n",
        "df.head()\n",
        "print(df.dtypes, '\\n')\n",
        "print(df[\"Type\"].unique(),'\\n')\n",
        "type = df[\"Type\"]\n",
        "type = df[\"Type\"].replace([\"Questionable\", \"Unconfirmed\", \"Unverified\", \"Invalid\", \"Under investigation\"], np.nan)\n",
        "type = type.replace([\"Watercraft\", \"Boating\", \"Sea Disaster\", \"Boat\", \"Boatomg\"],\"Watercraft_Related\")\n",
        "df[\"Type\"] = type\n",
        "print(df[\"Type\"].unique(),'\\n')\n",
        "del type"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a098254d",
      "metadata": {},
      "source": [
        "I combine [\"Questionable\", \"Unconfirmed\", \"Unverified\", \"Invalid\", \"Under investigation\"] together as  nan because those are all uncertain situation. Then I combine [\"Watercraft\", \"Boating\", \"Sea Disaster\", \"Boat\", \"Boatomg\"] to \"Watercraft_Related\" because these are all the types related to watercraft. After the cleaning process, I have 4 vairables for the type, which are ['Unprovoked' 'Provoked' nan 'Watercraft_Related'] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "75217e44",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before cleaning:  [9 0 1] \n",
            "\n",
            "After cleaning:  [nan  0.  1.] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Q2-3\n",
        "url = 'http://www.vcsc.virginia.gov/pretrialdataproject/October%202017%20Cohort_Virginia%20Pretrial%20Data%20Project_Deidentified%20FINAL%20Update_10272021.csv'\n",
        "df = pd.read_csv(url,low_memory=False)\n",
        "de_release = df['WhetherDefendantWasReleasedPretrial']\n",
        "print(\"Before cleaning: \", de_release.unique(),'\\n')\n",
        "de_release = de_release.replace(9,np.nan)\n",
        "print(\"After cleaning: \", de_release.unique(),'\\n')\n",
        "df['WhetherDefendantWasReleasedPretrial'] = de_release\n",
        "del de_release"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "b41a9fce",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of nan number Before:  9053 \n",
            "\n",
            "SentenceTypeAllChargesAtConvictionInContactEvent     0     1    2     4    9\n",
            "ImposedSentenceAllChargeInContactEvent                                      \n",
            "False                                             8720  4299  914     0    0\n",
            "True                                                 0     0    0  8779  274 \n",
            "\n",
            "Number of nan number After:  274 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Q2-4\n",
        "length = df[\"ImposedSentenceAllChargeInContactEvent\"]\n",
        "type = df[\"SentenceTypeAllChargesAtConvictionInContactEvent\"]\n",
        "\n",
        "# print(length.unique(),\"\\n\")\n",
        "length = pd.to_numeric(length, errors=\"coerce\")\n",
        "print(\"Number of nan value Before: \", sum(length.isnull()), \"\\n\")\n",
        "lengthisnull = length.isnull()\n",
        "print(pd.crosstab(lengthisnull,type),\"\\n\")\n",
        "\n",
        "length = length.mask(type == 4, 0)\n",
        "\n",
        "print(\"Number of nan value After: \", sum(length.isnull()), \"\\n\")\n",
        "\n",
        "df['ImposedSentenceAllChargeInContactEvent'] = length\n",
        "\n",
        "del length, type"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f233a52",
      "metadata": {},
      "source": [
        "I start with finding the type of \"ImposedSentenceAllChargeInContactEvent\" and I found that the type is string instead of float, so the first step I did is to change the type from string to numerical value using \"pd.to_numeric\". Then I applied sum to find out the number of nan number and realized that result if not ideal due to the hugh nan number. Given the hint I used \"crosstab\" to find the relationship between \"SentenceTypeAllChargesAtConvictionInContactEvent\" and \"ImposedSentenceAllChargeInContactEvent\". I found that most nan values occurs when \"SentenceTypeAllChargesAtConvictionInContactEvent\" is 4. Therefore, I masked the value based on the condition (\"SentenceTypeAllChargesAtConvictionInContactEvent\" = 4). The number of nan value after reduces to 274."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5",
      "metadata": {
        "id": "649494cd-cfd6-4f80-992a-9994fc19e1d5"
      },
      "source": [
        "**Q3.** Many important datasets contain a race variable, typically limited to a handful of values often including Black, White, Asian, Latino, and Indigenous. This question looks at data gathering efforts on this variable by the U.S. Federal government.\n",
        "\n",
        "1. How did the most recent US Census gather data on race?\n",
        "2. Why do we gather these data? What role do these kinds of data play in politics and society? Why does data quality matter?\n",
        "3. Please provide a constructive criticism of how the Census was conducted: What was done well? What do you think was missing? How should future large scale surveys be adjusted to best reflect the diversity of the population? Could some of the Census' good practices be adopted more widely to gather richer and more useful data?\n",
        "4. How did the Census gather data on sex and gender? Please provide a similar constructive criticism of their practices.\n",
        "5. When it comes to cleaning data, what concerns do you have about protected characteristics like sex, gender, sexual identity, or race? What challenges can you imagine arising when there are missing values? What good or bad practices might people adopt, and why?\n",
        "6. Suppose someone invented an algorithm to impute values for protected characteristics like race, gender, sex, or sexuality. What kinds of concerns would you have?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
